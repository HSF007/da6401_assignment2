{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9602babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e4fb242",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:19:15.273253Z",
     "iopub.status.busy": "2025-04-18T12:19:15.272998Z",
     "iopub.status.idle": "2025-04-18T12:19:18.186114Z",
     "shell.execute_reply": "2025-04-18T12:19:18.185394Z"
    },
    "papermill": {
     "duration": 2.917045,
     "end_time": "2025-04-18T12:19:18.187365",
     "exception": false,
     "start_time": "2025-04-18T12:19:15.270320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: da24m008 (da24m008-iit-madras) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13e5d0",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8fb77a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-18T12:19:18.192702Z",
     "iopub.status.busy": "2025-04-18T12:19:18.192463Z",
     "iopub.status.idle": "2025-04-18T12:19:22.204597Z",
     "shell.execute_reply": "2025-04-18T12:19:22.203790Z"
    },
    "papermill": {
     "duration": 4.016318,
     "end_time": "2025-04-18T12:19:22.205960",
     "exception": false,
     "start_time": "2025-04-18T12:19:18.189642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels=3,\n",
    "        filters_per_layer=[32, 64, 128, 256, 512],\n",
    "        kernel_size=3,\n",
    "        pool_sizes=2,\n",
    "        conv_activation='relu',\n",
    "        dense_units=256,\n",
    "        dense_activation='relu',\n",
    "        num_classes=10,\n",
    "        dropout_rate=0.5,\n",
    "        use_batch_norm=True\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_sizes = pool_sizes\n",
    "        self.conv_activation = conv_activation\n",
    "        self.dense_activation = dense_activation\n",
    "\n",
    "        # Initializing Convolutional, Batch Norm, and pooling layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.batch_norm_layers = nn.ModuleList()\n",
    "        self.pool_layers = nn.ModuleList()\n",
    "        \n",
    "        in_channels = input_channels\n",
    "        \n",
    "        # Create 5 convolutional blocks\n",
    "        for filters in filters_per_layer:\n",
    "            # Convolutional layer\n",
    "            self.conv_layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=\"same\",\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Batch normalization layer\n",
    "            if use_batch_norm:\n",
    "                self.batch_norm_layers.append(nn.BatchNorm2d(filters))\n",
    "            else:\n",
    "                self.batch_norm_layers.append(None)\n",
    "            \n",
    "            # Max pooling layer\n",
    "            self.pool_layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            \n",
    "            # Update in_channels for next layer\n",
    "            in_channels = filters\n",
    "        \n",
    "        # Calculate the output size after conv layers\n",
    "        # Assuming input is 224x224, after 5 max-pooling layers it will be 7x7\n",
    "        conv_output_size = 7 * 7 * filters_per_layer[-1]\n",
    "        \n",
    "        # First flatten the image to pass it to the dense layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Dense layer\n",
    "        self.fc1 = nn.Linear(conv_output_size, dense_units)\n",
    "        self.fc_bn = nn.BatchNorm1d(dense_units) if use_batch_norm else None\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc2 = nn.Linear(dense_units, num_classes)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def activation_func(self, activation, x):\n",
    "        \"\"\"Apply the selected activation function\"\"\"\n",
    "        if activation.lower() == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif activation.lower() == 'gelu':\n",
    "            return F.gelu(x)\n",
    "        elif activation.lower() == 'silu' or activation.lower() == 'swish':\n",
    "            return F.silu(x)\n",
    "        elif activation.lower() == 'mish':\n",
    "            return x * torch.tanh(F.softplus(x))\n",
    "            return F.sigmoid(x)\n",
    "        elif activation.lower() == 'leakyrelu':\n",
    "            return F.leaky_relu(x, negative_slope=0.01)\n",
    "        else:\n",
    "            # Default to ReLU\n",
    "            return F.relu(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # Convolutional blocks\n",
    "        for i, (conv, bn, pool) in enumerate(zip(self.conv_layers, self.batch_norm_layers, self.pool_layers)):\n",
    "            x = conv(x)\n",
    "            if bn is not None:\n",
    "                x = bn(x)\n",
    "            x = self.activation_func(self.conv_activation, x)\n",
    "            x = pool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Dense layer\n",
    "        x = self.fc1(x)\n",
    "        if self.fc_bn is not None:\n",
    "            x = self.fc_bn(x)\n",
    "        x = self.activation_func(self.dense_activation, x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1528971a",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9fca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Data paths\n",
    "DATASET_PATH = r\"/kaggle/input/inatural-12k/inaturalist_12K\"  # Update with your path\n",
    "TRAIN_DIR = os.path.join(DATASET_PATH, \"train\")\n",
    "TEST_DIR = os.path.join(DATASET_PATH, \"val\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7adcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'val_accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'base_filters': {'values': [32, 64, None]},\n",
    "        'conv_activation': {'values': ['relu', 'gelu', 'silu', 'mish']},\n",
    "        'dense_activation': {'values':['relu', 'gelu', 'silu', 'leakyrelu']},\n",
    "        'filter_organization': {'values': ['same', 'doubling', 'halving']},\n",
    "        'data_augmentation': {'values': [True, False]},\n",
    "        'use_batch_norm': {'values': [True, False]},\n",
    "        'dropout_rate': {'values': [0.2, 0.3, 0.5]},\n",
    "        'dense_neurons': {'values': [128, 256, 512]},\n",
    "        'learning_rate': {'values': [0.0001, 0.001]},\n",
    "        'epochs': {'value': 10},\n",
    "        'batch_size': {'value': 32},\n",
    "        'image_size': {'value': 224},\n",
    "        'validation_split': {'value': 0.2}\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab637974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class iNaturalistDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for iNaturalist images.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with class subdirectories.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get class directories and create class-to-idx mapping\n",
    "        self.classes = sorted([d for d in os.listdir(root_dir) \n",
    "                              if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        # Get all image paths and corresponding labels\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.image_paths.append(os.path.join(class_dir, img_name))\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc1e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_data(config):\n",
    "    \"\"\"\n",
    "    Load data and split into train and validation sets,\n",
    "    ensuring equal class representation in validation set.\n",
    "    \"\"\"\n",
    "    # Base transforms\n",
    "    base_transform = transforms.Compose([\n",
    "        transforms.Resize((config.image_size, config.image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Data augmentation transform\n",
    "    augment_transform = transforms.Compose([\n",
    "        transforms.Resize((config.image_size, config.image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Choose transform based on config\n",
    "    train_transform = augment_transform if config.data_augmentation else base_transform\n",
    "    \n",
    "    # Load dataset\n",
    "    full_dataset = iNaturalistDataset(root_dir=TRAIN_DIR, transform=train_transform)\n",
    "    val_dataset = iNaturalistDataset(root_dir=TRAIN_DIR, transform=base_transform)\n",
    "    test_datase = iNaturalistDataset(root_dir=TEST_DIR, transform=base_transform)\n",
    "    \n",
    "    # Get class counts for stratified split\n",
    "    class_counts = {}\n",
    "    for label in full_dataset.labels:\n",
    "        if label not in class_counts:\n",
    "            class_counts[label] = 0\n",
    "        class_counts[label] += 1\n",
    "    \n",
    "    # Create stratified split\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    \n",
    "    for class_idx in range(len(full_dataset.classes)):\n",
    "        # Get indices for this class\n",
    "        class_indices = [i for i, label in enumerate(full_dataset.labels) if label == class_idx]\n",
    "        np.random.shuffle(class_indices)\n",
    "        \n",
    "        # Split indices\n",
    "        val_count = int(len(class_indices) * config.validation_split)\n",
    "        val_indices.extend(class_indices[:val_count])\n",
    "        train_indices.extend(class_indices[val_count:])\n",
    "    \n",
    "    # Create subset datasets\n",
    "    train_dataset = Subset(full_dataset, train_indices)\n",
    "    val_dataset = Subset(val_dataset, val_indices)\n",
    "    \n",
    "    print(f\"Total training samples: {len(train_dataset)}\")\n",
    "    print(f\"Total validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_datase,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, len(full_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a48663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \"\"\"Train the model with current hyperparameter configuration.\"\"\"\n",
    "    # Load and split data\n",
    "    train_loader, val_loader, test_loader, num_classes = load_and_split_data(config)\n",
    "    \n",
    "    # Create model based on hyperparameters\n",
    "    if config.filter_organization == 'same' and config.base_filters:\n",
    "        filters = [config.base_filters] * 5\n",
    "    elif config.filter_organization == 'doubling' and config.base_filters:\n",
    "        filters = [config.base_filters * (2**i) if config.base_filters * (2**i) < 512 else 512 for i in range(5)]\n",
    "    elif config.filter_organization == 'halving' and config.base_filters:\n",
    "        filters = [config.base_filters * (2**(4-i)) if config.base_filters * (2**(4-i)) < 512 else 512 for i in range(5)]\n",
    "    else:\n",
    "        filters = [32, 64, 128, 256, 512]  # Default\n",
    "    \n",
    "    model = CNNModel(\n",
    "        input_channels=3,\n",
    "        num_classes=num_classes,\n",
    "        filters_per_layer=filters,\n",
    "        kernel_size=3,\n",
    "        conv_activation=config.conv_activation,\n",
    "        dense_units=config.dense_neurons,\n",
    "        dense_activation = config.dense_activation,\n",
    "        dropout_rate=config.dropout_rate,\n",
    "        use_batch_norm=config.use_batch_norm\n",
    "    )\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    # Initialize WandB for tracking\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=100)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": running_loss / len(train_loader),\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"val_loss\": val_loss / len(val_loader),\n",
    "            \"val_accuracy\": val_acc\n",
    "        })\n",
    "        \n",
    "        print(f'Epoch: {epoch + 1}, Val Loss: {val_loss / len(val_loader):.3f}, Val Acc: {100 * val_acc:.2f}%')\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            # Save the model\n",
    "            torch.save(model.state_dict(), f\"best_model_{wandb.run.id}.pth\")\n",
    "            # Log the model to wandb\n",
    "            artifact = wandb.Artifact('model', type='model')\n",
    "            artifact.add_file(f\"best_model_{wandb.run.id}.pth\")\n",
    "            wandb.log_artifact(artifact)\n",
    "    \n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    test_acc = test_correct / test_total\n",
    "    print(f'Test Accuracy: {100 * test_acc:.2f}%')\n",
    "    wandb.log({\"test_accuracy\": test_acc})\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), f\"final_model_{wandb.run.id}.pth\")\n",
    "\n",
    "    # Log the model to wandb\n",
    "    artifact = wandb.Artifact('model', type='model')\n",
    "    artifact.add_file(f\"final_model_{wandb.run.id}.pth\")\n",
    "    wandb.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4860ffa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:19:22.222640Z",
     "iopub.status.busy": "2025-04-18T12:19:22.222429Z",
     "iopub.status.idle": "2025-04-18T12:19:27.095459Z",
     "shell.execute_reply": "2025-04-18T12:19:27.094622Z"
    },
    "papermill": {
     "duration": 4.877264,
     "end_time": "2025-04-18T12:19:27.096826",
     "exception": false,
     "start_time": "2025-04-18T12:19:22.219562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "def sweep_train():\n",
    "    \"\"\"Configure and run hyperparameter sweep.\"\"\"\n",
    "    # Initialize wandb\n",
    "    wandb.init()\n",
    "\n",
    "    # Configuration parameters\n",
    "    config = wandb.config\n",
    "\n",
    "    # Set run name based on hyperparameters\n",
    "    run_name = f'bf_{config.base_filters}_fo_{config.filter_organization}_dn_{config.dense_neurons}_ca_{config.conv_activation}_da_{config.dense_activation}_v5'\n",
    "    wandb.run.name = run_name\n",
    "\n",
    "    # Call training function with current hyperparameters\n",
    "    train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b89ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:19:27.102392Z",
     "iopub.status.busy": "2025-04-18T12:19:27.102018Z",
     "iopub.status.idle": "2025-04-18T18:07:31.301048Z",
     "shell.execute_reply": "2025-04-18T18:07:31.300223Z"
    },
    "papermill": {
     "duration": 20884.203333,
     "end_time": "2025-04-18T18:07:31.302527",
     "exception": false,
     "start_time": "2025-04-18T12:19:27.099194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: owlu7slq\n",
      "Sweep URL: https://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1r5jogya with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: halving\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_121928-1r5jogya\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlyric-sweep-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/1r5jogya\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.113, Val Acc: 25.41%\n",
      "Epoch: 2, Val Loss: 2.091, Val Acc: 27.16%\n",
      "Epoch: 3, Val Loss: 1.982, Val Acc: 29.86%\n",
      "Epoch: 4, Val Loss: 1.945, Val Acc: 30.62%\n",
      "Epoch: 5, Val Loss: 1.945, Val Acc: 32.37%\n",
      "Epoch: 6, Val Loss: 2.034, Val Acc: 32.02%\n",
      "Epoch: 7, Val Loss: 2.459, Val Acc: 29.06%\n",
      "Epoch: 8, Val Loss: 2.943, Val Acc: 29.46%\n",
      "Epoch: 9, Val Loss: 3.723, Val Acc: 30.27%\n",
      "Epoch: 10, Val Loss: 4.955, Val Acc: 29.06%\n",
      "Test Accuracy: 32.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▂▂▃▄▅▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▃▅▆██▅▅▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▁▁▁▁▁▁▂▃▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.327\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.9205\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.29065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 4.95543\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_None_fo_halving_dn_512_ca_silu_da_silu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/1r5jogya\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 12 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_121928-1r5jogya/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jof4jo9i with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_122838-jof4jo9i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mabsurd-sweep-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/jof4jo9i\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.068, Val Acc: 26.76%\n",
      "Epoch: 2, Val Loss: 1.978, Val Acc: 30.92%\n",
      "Epoch: 3, Val Loss: 1.904, Val Acc: 33.32%\n",
      "Epoch: 4, Val Loss: 1.906, Val Acc: 32.72%\n",
      "Epoch: 5, Val Loss: 1.878, Val Acc: 35.97%\n",
      "Epoch: 6, Val Loss: 1.827, Val Acc: 36.27%\n",
      "Epoch: 7, Val Loss: 1.812, Val Acc: 37.17%\n",
      "Epoch: 8, Val Loss: 1.884, Val Acc: 36.47%\n",
      "Epoch: 9, Val Loss: 1.854, Val Acc: 37.02%\n",
      "Epoch: 10, Val Loss: 1.939, Val Acc: 36.97%\n",
      "Test Accuracy: 36.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading wandb-summary.json; uploading config.yaml; uploading history steps 9-10, summary, console lines 11-12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▃▄▄▅▅▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▄▅▅▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▄▄▃▁▁▃▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.3665\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.55688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.36968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.93855\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_None_fo_doubling_dn_256_ca_gelu_da_gelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/jof4jo9i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_122838-jof4jo9i/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cmzelwql with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: halving\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_123701-cmzelwql\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwoven-sweep-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/cmzelwql\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 1.962, Val Acc: 31.27%\n",
      "Epoch: 2, Val Loss: 1.901, Val Acc: 32.47%\n",
      "Epoch: 3, Val Loss: 1.912, Val Acc: 32.12%\n",
      "Epoch: 4, Val Loss: 1.802, Val Acc: 36.17%\n",
      "Epoch: 5, Val Loss: 1.788, Val Acc: 37.57%\n",
      "Epoch: 6, Val Loss: 1.798, Val Acc: 37.52%\n",
      "Epoch: 7, Val Loss: 1.765, Val Acc: 38.77%\n",
      "Epoch: 8, Val Loss: 1.812, Val Acc: 38.92%\n",
      "Epoch: 9, Val Loss: 1.930, Val Acc: 37.12%\n",
      "Epoch: 10, Val Loss: 1.941, Val Acc: 36.82%\n",
      "Test Accuracy: 37.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▃▃▄▅▅▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▂▂▅▇▇██▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▆▂▂▂▁▃▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.3725\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.62062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.36818\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.9414\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_None_fo_halving_dn_256_ca_mish_da_silu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/cmzelwql\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_123701-cmzelwql/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5dn5v1bo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_124544-5dn5v1bo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msolar-sweep-4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/5dn5v1bo\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.006, Val Acc: 28.21%\n",
      "Epoch: 2, Val Loss: 1.982, Val Acc: 29.66%\n",
      "Epoch: 3, Val Loss: 1.873, Val Acc: 34.02%\n",
      "Epoch: 4, Val Loss: 1.849, Val Acc: 34.67%\n",
      "Epoch: 5, Val Loss: 1.809, Val Acc: 36.77%\n",
      "Epoch: 6, Val Loss: 1.791, Val Acc: 37.27%\n",
      "Epoch: 7, Val Loss: 1.839, Val Acc: 34.02%\n",
      "Epoch: 8, Val Loss: 1.860, Val Acc: 34.32%\n",
      "Epoch: 9, Val Loss: 1.754, Val Acc: 38.07%\n",
      "Epoch: 10, Val Loss: 1.708, Val Acc: 40.62%\n",
      "Test Accuracy: 40.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading wandb-summary.json; uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▅▅▆▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▂▄▅▆▆▄▄▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▇▅▄▃▃▄▅▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.4045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.42087\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.4062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.70814\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_None_fo_same_dn_128_ca_gelu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/5dn5v1bo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 18 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_124544-5dn5v1bo/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n4rcmls5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_125648-n4rcmls5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33myouthful-sweep-5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/n4rcmls5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 1.965, Val Acc: 31.57%\n",
      "Epoch: 2, Val Loss: 1.873, Val Acc: 34.27%\n",
      "Epoch: 3, Val Loss: 1.883, Val Acc: 33.57%\n",
      "Epoch: 4, Val Loss: 1.826, Val Acc: 35.87%\n",
      "Epoch: 5, Val Loss: 1.878, Val Acc: 35.32%\n",
      "Epoch: 6, Val Loss: 1.839, Val Acc: 38.07%\n",
      "Epoch: 7, Val Loss: 1.888, Val Acc: 34.62%\n",
      "Epoch: 8, Val Loss: 1.882, Val Acc: 37.92%\n",
      "Epoch: 9, Val Loss: 1.973, Val Acc: 37.02%\n",
      "Epoch: 10, Val Loss: 2.007, Val Acc: 37.67%\n",
      "Test Accuracy: 39.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▃▄▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▄▃▆▅█▄█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▆▃▃▁▃▂▃▃▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.3965\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.6405\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.37669\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.00684\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_32_fo_same_dn_128_ca_silu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/n4rcmls5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_125648-n4rcmls5/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gx693ygj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_130510-gx693ygj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msolar-sweep-6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/gx693ygj\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.091, Val Acc: 28.26%\n",
      "Epoch: 2, Val Loss: 1.880, Val Acc: 34.22%\n",
      "Epoch: 3, Val Loss: 1.813, Val Acc: 37.62%\n",
      "Epoch: 4, Val Loss: 1.791, Val Acc: 36.67%\n",
      "Epoch: 5, Val Loss: 1.755, Val Acc: 37.17%\n",
      "Epoch: 6, Val Loss: 1.803, Val Acc: 37.97%\n",
      "Epoch: 7, Val Loss: 1.751, Val Acc: 39.17%\n",
      "Epoch: 8, Val Loss: 1.788, Val Acc: 38.12%\n",
      "Epoch: 9, Val Loss: 1.800, Val Acc: 39.57%\n",
      "Epoch: 10, Val Loss: 1.837, Val Acc: 39.42%\n",
      "Test Accuracy: 39.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▃▄▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▅▇▆▇▇█▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▄▂▂▁▂▁▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.3945\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.57675\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.3942\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.83675\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_32_fo_same_dn_256_ca_gelu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/gx693ygj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_130510-gx693ygj/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d56qhdi5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_131329-d56qhdi5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33matomic-sweep-7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/d56qhdi5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.020, Val Acc: 28.06%\n",
      "Epoch: 2, Val Loss: 1.962, Val Acc: 30.17%\n",
      "Epoch: 3, Val Loss: 2.003, Val Acc: 29.16%\n",
      "Epoch: 4, Val Loss: 1.907, Val Acc: 31.92%\n",
      "Epoch: 5, Val Loss: 1.799, Val Acc: 36.27%\n",
      "Epoch: 6, Val Loss: 1.806, Val Acc: 35.82%\n",
      "Epoch: 7, Val Loss: 1.778, Val Acc: 37.02%\n",
      "Epoch: 8, Val Loss: 1.861, Val Acc: 33.77%\n",
      "Epoch: 9, Val Loss: 1.894, Val Acc: 34.37%\n",
      "Epoch: 10, Val Loss: 1.749, Val Acc: 39.02%\n",
      "Test Accuracy: 42.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▅▆▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▂▂▃▆▆▇▅▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆█▅▂▂▂▄▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.422\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.41837\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.3902\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.74865\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_64_fo_doubling_dn_256_ca_relu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/d56qhdi5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_131329-d56qhdi5/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0npit9eb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_132504-0npit9eb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mglamorous-sweep-8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/0npit9eb\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.029, Val Acc: 28.71%\n",
      "Epoch: 2, Val Loss: 2.001, Val Acc: 28.36%\n",
      "Epoch: 3, Val Loss: 1.954, Val Acc: 30.87%\n",
      "Epoch: 4, Val Loss: 1.923, Val Acc: 32.62%\n",
      "Epoch: 5, Val Loss: 1.855, Val Acc: 35.52%\n",
      "Epoch: 6, Val Loss: 1.807, Val Acc: 35.17%\n",
      "Epoch: 7, Val Loss: 1.870, Val Acc: 34.17%\n",
      "Epoch: 8, Val Loss: 1.782, Val Acc: 37.92%\n",
      "Epoch: 9, Val Loss: 1.849, Val Acc: 36.72%\n",
      "Epoch: 10, Val Loss: 1.763, Val Acc: 38.52%\n",
      "Test Accuracy: 42.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▅▆▆▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▁▃▄▆▆▅█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▇▆▅▃▂▄▁▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.4215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.42575\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.38519\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.76261\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_64_fo_doubling_dn_128_ca_gelu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/0npit9eb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_132504-0npit9eb/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wvyptlyf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_133625-wvyptlyf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mefficient-sweep-9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/wvyptlyf\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.076, Val Acc: 25.26%\n",
      "Epoch: 2, Val Loss: 1.964, Val Acc: 28.21%\n",
      "Epoch: 3, Val Loss: 1.910, Val Acc: 32.17%\n",
      "Epoch: 4, Val Loss: 1.870, Val Acc: 33.82%\n",
      "Epoch: 5, Val Loss: 1.819, Val Acc: 36.07%\n",
      "Epoch: 6, Val Loss: 1.842, Val Acc: 35.27%\n",
      "Epoch: 7, Val Loss: 1.765, Val Acc: 39.27%\n",
      "Epoch: 8, Val Loss: 1.798, Val Acc: 36.82%\n",
      "Epoch: 9, Val Loss: 1.719, Val Acc: 39.42%\n",
      "Epoch: 10, Val Loss: 1.748, Val Acc: 38.07%\n",
      "Test Accuracy: 38.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▄▄▅▆▆▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▂▄▅▆▆█▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▅▄▃▃▂▃▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.381\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.37375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.38069\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.74845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_32_fo_doubling_dn_128_ca_gelu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/wvyptlyf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 16 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_133625-wvyptlyf/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m2xzcoi5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_134718-m2xzcoi5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrich-sweep-10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/m2xzcoi5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.186, Val Acc: 20.71%\n",
      "Epoch: 2, Val Loss: 2.109, Val Acc: 22.81%\n",
      "Epoch: 3, Val Loss: 2.046, Val Acc: 24.51%\n",
      "Epoch: 4, Val Loss: 2.018, Val Acc: 27.56%\n",
      "Epoch: 5, Val Loss: 1.946, Val Acc: 31.27%\n",
      "Epoch: 6, Val Loss: 1.896, Val Acc: 32.62%\n",
      "Epoch: 7, Val Loss: 1.921, Val Acc: 33.32%\n",
      "Epoch: 8, Val Loss: 1.925, Val Acc: 34.92%\n",
      "Epoch: 9, Val Loss: 1.956, Val Acc: 35.17%\n",
      "Epoch: 10, Val Loss: 1.898, Val Acc: 35.22%\n",
      "Test Accuracy: 35.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▃▄▅▅▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▂▃▄▆▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▅▄▂▁▂▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.3545\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.44175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.35218\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.89764\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_64_fo_same_dn_128_ca_relu_da_silu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/m2xzcoi5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 22 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_134718-m2xzcoi5/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0osq1vhe with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_135536-0osq1vhe\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msweet-sweep-11\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/0osq1vhe\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.312, Val Acc: 11.81%\n",
      "Epoch: 2, Val Loss: 2.300, Val Acc: 12.96%\n",
      "Epoch: 3, Val Loss: 2.255, Val Acc: 13.86%\n",
      "Epoch: 4, Val Loss: 2.245, Val Acc: 13.11%\n",
      "Epoch: 5, Val Loss: 2.166, Val Acc: 21.66%\n",
      "Epoch: 6, Val Loss: 2.186, Val Acc: 19.56%\n",
      "Epoch: 7, Val Loss: 2.148, Val Acc: 22.71%\n",
      "Epoch: 8, Val Loss: 2.171, Val Acc: 20.36%\n",
      "Epoch: 9, Val Loss: 2.123, Val Acc: 22.46%\n",
      "Epoch: 10, Val Loss: 2.080, Val Acc: 27.16%\n",
      "Test Accuracy: 26.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▂▁▂▃▄▆▆▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▂▂▂▅▅▆▅▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ██▆▆▄▄▃▄▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.2645\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.21388\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.27164\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.07956\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_64_fo_doubling_dn_128_ca_silu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/0osq1vhe\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_135536-0osq1vhe/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5zgaoqny with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_140654-5zgaoqny\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgenial-sweep-12\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/5zgaoqny\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.062, Val Acc: 24.51%\n",
      "Epoch: 2, Val Loss: 1.984, Val Acc: 28.81%\n",
      "Epoch: 3, Val Loss: 1.928, Val Acc: 31.77%\n",
      "Epoch: 4, Val Loss: 1.900, Val Acc: 32.97%\n",
      "Epoch: 5, Val Loss: 1.855, Val Acc: 36.17%\n",
      "Epoch: 6, Val Loss: 1.818, Val Acc: 36.92%\n",
      "Epoch: 7, Val Loss: 1.813, Val Acc: 37.12%\n",
      "Epoch: 8, Val Loss: 1.845, Val Acc: 37.57%\n",
      "Epoch: 9, Val Loss: 1.826, Val Acc: 37.62%\n",
      "Epoch: 10, Val Loss: 1.928, Val Acc: 37.47%\n",
      "Test Accuracy: 37.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▃▄▄▅▅▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▃▅▆▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▄▃▂▁▁▂▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.374\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.56037\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.37469\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.92816\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_32_fo_doubling_dn_256_ca_relu_da_silu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/5zgaoqny\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 20 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_140654-5zgaoqny/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2wix17w8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_141512-2wix17w8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mswift-sweep-13\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/2wix17w8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.177, Val Acc: 19.31%\n",
      "Epoch: 2, Val Loss: 2.095, Val Acc: 23.21%\n",
      "Epoch: 3, Val Loss: 2.104, Val Acc: 23.01%\n",
      "Epoch: 4, Val Loss: 2.034, Val Acc: 27.56%\n",
      "Epoch: 5, Val Loss: 1.981, Val Acc: 28.91%\n",
      "Epoch: 6, Val Loss: 1.916, Val Acc: 33.07%\n",
      "Epoch: 7, Val Loss: 1.922, Val Acc: 30.97%\n",
      "Epoch: 8, Val Loss: 1.910, Val Acc: 33.37%\n",
      "Epoch: 9, Val Loss: 1.861, Val Acc: 34.47%\n",
      "Epoch: 10, Val Loss: 1.866, Val Acc: 34.37%\n",
      "Test Accuracy: 35.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▅▅▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▃▃▅▅▇▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▆▅▄▂▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.353\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.35513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.34367\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.86627\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_None_fo_same_dn_128_ca_relu_da_gelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/2wix17w8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 16 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_141512-2wix17w8/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xbu9mzwk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: halving\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_142606-xbu9mzwk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrose-sweep-14\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/xbu9mzwk\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_32_fo_halving_dn_512_ca_silu_da_silu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/xbu9mzwk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_142606-xbu9mzwk/logs\u001b[0m\n",
      "Run xbu9mzwk errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_19/1168500401.py\", line 326, in sweep_train\n",
      "    train_model(config)\n",
      "  File \"/tmp/ipykernel_19/1168500401.py\", line 238, in train_model\n",
      "    loss.backward()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 15.89 GiB of which 2.16 GiB is free. Process 3109 has 13.73 GiB memory in use. Of the allocated memory 11.54 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run xbu9mzwk errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/1168500401.py\", line 326, in sweep_train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_model(config)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/1168500401.py\", line 238, in train_model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss.backward()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     torch.autograd.backward(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     _engine_run_backward(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 15.89 GiB of which 2.16 GiB is free. Process 3109 has 13.73 GiB memory in use. Of the allocated memory 11.54 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: su56nvzy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_142616-su56nvzy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mquiet-sweep-15\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/su56nvzy\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.114, Val Acc: 24.76%\n",
      "Epoch: 2, Val Loss: 2.036, Val Acc: 26.86%\n",
      "Epoch: 3, Val Loss: 2.026, Val Acc: 29.06%\n",
      "Epoch: 4, Val Loss: 1.977, Val Acc: 31.27%\n",
      "Epoch: 5, Val Loss: 1.962, Val Acc: 31.37%\n",
      "Epoch: 6, Val Loss: 1.962, Val Acc: 31.02%\n",
      "Epoch: 7, Val Loss: 1.909, Val Acc: 34.17%\n",
      "Epoch: 8, Val Loss: 1.892, Val Acc: 34.27%\n",
      "Epoch: 9, Val Loss: 1.866, Val Acc: 35.42%\n",
      "Epoch: 10, Val Loss: 1.903, Val Acc: 34.12%\n",
      "Test Accuracy: 33.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▄▄▅▆▆▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▂▄▅▅▅▇▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▆▄▄▄▂▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.339\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.32775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.34117\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.90274\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_None_fo_doubling_dn_128_ca_relu_da_relu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/su56nvzy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 18 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_142616-su56nvzy/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eu8dladw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_143710-eu8dladw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwoven-sweep-16\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/eu8dladw\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 1.863, Val Acc: 33.12%\n",
      "Epoch: 2, Val Loss: 1.848, Val Acc: 34.92%\n",
      "Epoch: 3, Val Loss: 1.840, Val Acc: 36.97%\n",
      "Epoch: 4, Val Loss: 2.034, Val Acc: 31.22%\n",
      "Epoch: 5, Val Loss: 2.213, Val Acc: 31.57%\n",
      "Epoch: 6, Val Loss: 2.254, Val Acc: 33.47%\n",
      "Epoch: 7, Val Loss: 2.167, Val Acc: 34.92%\n",
      "Epoch: 8, Val Loss: 2.168, Val Acc: 34.27%\n",
      "Epoch: 9, Val Loss: 2.238, Val Acc: 34.57%\n",
      "Epoch: 10, Val Loss: 2.167, Val Acc: 36.12%\n",
      "Test Accuracy: 35.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▃▅▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▃▆█▁▁▄▆▅▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▁▁▁▄▇█▇▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.356\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.99938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.36118\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.16743\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_None_fo_doubling_dn_512_ca_relu_da_silu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/eu8dladw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_143710-eu8dladw/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cwz3qeib with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_144544-cwz3qeib\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdriven-sweep-17\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/cwz3qeib\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.055, Val Acc: 27.46%\n",
      "Epoch: 2, Val Loss: 1.988, Val Acc: 29.91%\n",
      "Epoch: 3, Val Loss: 1.940, Val Acc: 33.47%\n",
      "Epoch: 4, Val Loss: 1.904, Val Acc: 34.47%\n",
      "Epoch: 5, Val Loss: 1.848, Val Acc: 36.17%\n",
      "Epoch: 6, Val Loss: 1.906, Val Acc: 36.02%\n",
      "Epoch: 7, Val Loss: 2.156, Val Acc: 34.72%\n",
      "Epoch: 8, Val Loss: 2.255, Val Acc: 35.12%\n",
      "Epoch: 9, Val Loss: 2.764, Val Acc: 35.62%\n",
      "Epoch: 10, Val Loss: 2.895, Val Acc: 35.92%\n",
      "Test Accuracy: 37.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▂▃▃▄▅▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▃▆▇██▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▂▂▂▁▁▁▃▄▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.3705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.91475\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.35918\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.89451\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_64_fo_same_dn_512_ca_gelu_da_gelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/cwz3qeib\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 12 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_144544-cwz3qeib/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0qnozmz8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: halving\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_145408-0qnozmz8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mflowing-sweep-18\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/0qnozmz8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_64_fo_halving_dn_256_ca_mish_da_silu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/0qnozmz8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_145408-0qnozmz8/logs\u001b[0m\n",
      "Run 0qnozmz8 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_19/1168500401.py\", line 326, in sweep_train\n",
      "    train_model(config)\n",
      "  File \"/tmp/ipykernel_19/1168500401.py\", line 234, in train_model\n",
      "    outputs = model(inputs)\n",
      "              ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1844, in _call_impl\n",
      "    return inner()\n",
      "           ^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1790, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_19/3824698835.py\", line 96, in forward\n",
      "    x = conv(x)\n",
      "        ^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 15.89 GiB of which 2.16 GiB is free. Process 3109 has 13.73 GiB memory in use. Of the allocated memory 10.81 GiB is allocated by PyTorch, and 2.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 0qnozmz8 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/1168500401.py\", line 326, in sweep_train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_model(config)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/1168500401.py\", line 234, in train_model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1844, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return inner()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1790, in inner\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     result = forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/3824698835.py\", line 96, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = conv(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 15.89 GiB of which 2.16 GiB is free. Process 3109 has 13.73 GiB memory in use. Of the allocated memory 10.81 GiB is allocated by PyTorch, and 2.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: utus44bu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_145423-utus44bu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlilac-sweep-19\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/utus44bu\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.087, Val Acc: 24.36%\n",
      "Epoch: 2, Val Loss: 2.061, Val Acc: 25.06%\n",
      "Epoch: 3, Val Loss: 1.921, Val Acc: 29.41%\n",
      "Epoch: 4, Val Loss: 1.869, Val Acc: 33.47%\n",
      "Epoch: 5, Val Loss: 1.847, Val Acc: 35.12%\n",
      "Epoch: 6, Val Loss: 1.872, Val Acc: 33.17%\n",
      "Epoch: 7, Val Loss: 1.794, Val Acc: 37.42%\n",
      "Epoch: 8, Val Loss: 1.770, Val Acc: 37.82%\n",
      "Epoch: 9, Val Loss: 1.771, Val Acc: 37.02%\n",
      "Epoch: 10, Val Loss: 1.771, Val Acc: 36.82%\n",
      "Test Accuracy: 38.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▅▆▆▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▁▄▆▇▆███▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▇▄▃▃▃▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.385\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.38338\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.36818\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.77139\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_64_fo_same_dn_256_ca_gelu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/utus44bu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 16 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_145423-utus44bu/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kfbyx55h with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_150527-kfbyx55h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msummer-sweep-20\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/kfbyx55h\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.019, Val Acc: 27.31%\n",
      "Epoch: 2, Val Loss: 1.917, Val Acc: 31.57%\n",
      "Epoch: 3, Val Loss: 1.925, Val Acc: 32.37%\n",
      "Epoch: 4, Val Loss: 1.813, Val Acc: 33.47%\n",
      "Epoch: 5, Val Loss: 1.792, Val Acc: 37.87%\n",
      "Epoch: 6, Val Loss: 1.793, Val Acc: 37.62%\n",
      "Epoch: 7, Val Loss: 1.795, Val Acc: 37.52%\n",
      "Epoch: 8, Val Loss: 1.724, Val Acc: 39.42%\n",
      "Epoch: 9, Val Loss: 1.698, Val Acc: 40.12%\n",
      "Epoch: 10, Val Loss: 1.702, Val Acc: 40.12%\n",
      "Test Accuracy: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▅▆▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▃▄▄▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▆▄▃▃▃▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.4115\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.40025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.4012\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.70208\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_32_fo_same_dn_128_ca_gelu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/kfbyx55h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 16 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_150527-kfbyx55h/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jqg88g6n with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_151611-jqg88g6n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mzany-sweep-21\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/jqg88g6n\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.079, Val Acc: 25.06%\n",
      "Epoch: 2, Val Loss: 2.107, Val Acc: 27.01%\n",
      "Epoch: 3, Val Loss: 1.942, Val Acc: 29.71%\n",
      "Epoch: 4, Val Loss: 2.069, Val Acc: 27.96%\n",
      "Epoch: 5, Val Loss: 1.936, Val Acc: 30.87%\n",
      "Epoch: 6, Val Loss: 1.869, Val Acc: 35.37%\n",
      "Epoch: 7, Val Loss: 1.789, Val Acc: 36.37%\n",
      "Epoch: 8, Val Loss: 1.785, Val Acc: 36.27%\n",
      "Epoch: 9, Val Loss: 1.762, Val Acc: 38.27%\n",
      "Epoch: 10, Val Loss: 1.792, Val Acc: 38.57%\n",
      "Test Accuracy: 40.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; updating run config\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▅▆▆▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▂▃▃▄▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▇█▅▇▅▃▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.403\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.41825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.38569\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.79247\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_64_fo_doubling_dn_256_ca_relu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/jqg88g6n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 18 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_151611-jqg88g6n/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 171n7452 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: halving\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_152748-171n7452\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msolar-sweep-22\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/171n7452\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_32_fo_halving_dn_256_ca_gelu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/171n7452\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_152748-171n7452/logs\u001b[0m\n",
      "Run 171n7452 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_19/1168500401.py\", line 326, in sweep_train\n",
      "    train_model(config)\n",
      "  File \"/tmp/ipykernel_19/1168500401.py\", line 234, in train_model\n",
      "    outputs = model(inputs)\n",
      "              ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1844, in _call_impl\n",
      "    return inner()\n",
      "           ^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1790, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_19/3824698835.py\", line 96, in forward\n",
      "    x = conv(x)\n",
      "        ^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.51 GiB is free. Process 3109 has 14.38 GiB memory in use. Of the allocated memory 10.83 GiB is allocated by PyTorch, and 3.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 171n7452 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/1168500401.py\", line 326, in sweep_train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_model(config)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/1168500401.py\", line 234, in train_model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1844, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return inner()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1790, in inner\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     result = forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/3824698835.py\", line 96, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = conv(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.51 GiB is free. Process 3109 has 14.38 GiB memory in use. Of the allocated memory 10.83 GiB is allocated by PyTorch, and 3.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4kg0h7id with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_152804-4kg0h7id\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mglamorous-sweep-23\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/4kg0h7id\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.191, Val Acc: 19.31%\n",
      "Epoch: 2, Val Loss: 2.127, Val Acc: 22.41%\n",
      "Epoch: 3, Val Loss: 2.049, Val Acc: 26.46%\n",
      "Epoch: 4, Val Loss: 1.981, Val Acc: 29.11%\n",
      "Epoch: 5, Val Loss: 1.982, Val Acc: 28.96%\n",
      "Epoch: 6, Val Loss: 1.952, Val Acc: 29.81%\n",
      "Epoch: 7, Val Loss: 1.917, Val Acc: 32.32%\n",
      "Epoch: 8, Val Loss: 1.883, Val Acc: 33.67%\n",
      "Epoch: 9, Val Loss: 1.877, Val Acc: 34.62%\n",
      "Epoch: 10, Val Loss: 1.862, Val Acc: 34.87%\n",
      "Test Accuracy: 34.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▅▆▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▂▄▅▅▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▇▅▄▄▃▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.349\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.336\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.34867\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.86213\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_32_fo_doubling_dn_128_ca_relu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/4kg0h7id\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 20 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_152804-4kg0h7id/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: baucoeeh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_153858-baucoeeh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrose-sweep-24\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/baucoeeh\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.077, Val Acc: 27.36%\n",
      "Epoch: 2, Val Loss: 1.980, Val Acc: 30.02%\n",
      "Epoch: 3, Val Loss: 1.953, Val Acc: 32.12%\n",
      "Epoch: 4, Val Loss: 1.944, Val Acc: 32.72%\n",
      "Epoch: 5, Val Loss: 1.827, Val Acc: 37.12%\n",
      "Epoch: 6, Val Loss: 1.805, Val Acc: 36.97%\n",
      "Epoch: 7, Val Loss: 1.857, Val Acc: 35.87%\n",
      "Epoch: 8, Val Loss: 1.768, Val Acc: 39.87%\n",
      "Epoch: 9, Val Loss: 1.797, Val Acc: 39.12%\n",
      "Epoch: 10, Val Loss: 1.868, Val Acc: 36.22%\n",
      "Test Accuracy: 38.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▅▆▆▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▂▄▄▆▆▆██▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▅▅▂▂▃▁▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.381\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.42525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.36218\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.86834\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_32_fo_same_dn_256_ca_gelu_da_gelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/baucoeeh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_153858-baucoeeh/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: itwk8k67 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_154946-itwk8k67\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mneat-sweep-25\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/itwk8k67\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 1.889, Val Acc: 32.87%\n",
      "Epoch: 2, Val Loss: 1.820, Val Acc: 37.17%\n",
      "Epoch: 3, Val Loss: 1.818, Val Acc: 36.97%\n",
      "Epoch: 4, Val Loss: 1.774, Val Acc: 39.87%\n",
      "Epoch: 5, Val Loss: 1.879, Val Acc: 35.97%\n",
      "Epoch: 6, Val Loss: 1.938, Val Acc: 34.92%\n",
      "Epoch: 7, Val Loss: 1.949, Val Acc: 35.77%\n",
      "Epoch: 8, Val Loss: 2.035, Val Acc: 35.12%\n",
      "Epoch: 9, Val Loss: 2.050, Val Acc: 35.82%\n",
      "Epoch: 10, Val Loss: 2.136, Val Acc: 34.32%\n",
      "Test Accuracy: 36.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; updating run config; uploading output.log; uploading wandb-summary.json\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▃▅▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▅▅█▄▃▄▃▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▃▂▂▁▃▄▄▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.362\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.9995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.34317\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.13584\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_None_fo_doubling_dn_256_ca_silu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/itwk8k67\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_154946-itwk8k67/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k5y4c38v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_155814-k5y4c38v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mglad-sweep-26\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/k5y4c38v\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.039, Val Acc: 28.06%\n",
      "Epoch: 2, Val Loss: 1.953, Val Acc: 31.12%\n",
      "Epoch: 3, Val Loss: 1.875, Val Acc: 33.27%\n",
      "Epoch: 4, Val Loss: 1.860, Val Acc: 32.17%\n",
      "Epoch: 5, Val Loss: 1.783, Val Acc: 36.92%\n",
      "Epoch: 6, Val Loss: 1.816, Val Acc: 36.07%\n",
      "Epoch: 7, Val Loss: 1.734, Val Acc: 38.92%\n",
      "Epoch: 8, Val Loss: 1.708, Val Acc: 40.62%\n",
      "Epoch: 9, Val Loss: 1.716, Val Acc: 39.72%\n",
      "Epoch: 10, Val Loss: 1.903, Val Acc: 35.47%\n",
      "Test Accuracy: 36.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▅▅▆▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▃▄▃▆▅▇█▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▅▄▃▃▂▁▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.366\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.41037\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.35468\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.90318\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_None_fo_same_dn_256_ca_silu_da_relu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/k5y4c38v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_155814-k5y4c38v/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vujgrhi8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_160919-vujgrhi8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtreasured-sweep-27\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/vujgrhi8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.143, Val Acc: 22.16%\n",
      "Epoch: 2, Val Loss: 2.038, Val Acc: 26.76%\n",
      "Epoch: 3, Val Loss: 1.982, Val Acc: 30.07%\n",
      "Epoch: 4, Val Loss: 2.003, Val Acc: 28.26%\n",
      "Epoch: 5, Val Loss: 1.967, Val Acc: 31.67%\n",
      "Epoch: 6, Val Loss: 1.898, Val Acc: 34.17%\n",
      "Epoch: 7, Val Loss: 1.871, Val Acc: 35.02%\n",
      "Epoch: 8, Val Loss: 1.845, Val Acc: 35.47%\n",
      "Epoch: 9, Val Loss: 1.826, Val Acc: 36.92%\n",
      "Epoch: 10, Val Loss: 1.806, Val Acc: 37.02%\n",
      "Test Accuracy: 35.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; updating run config\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading wandb-summary.json; uploading history steps 9-10, summary, console lines 11-12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▅▆▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▃▅▄▅▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▅▅▄▃▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.359\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.33813\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.37019\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.80552\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_None_fo_same_dn_128_ca_relu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/vujgrhi8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 20 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_160919-vujgrhi8/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 23drazuo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_162013-23drazuo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mamber-sweep-28\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/23drazuo\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.231, Val Acc: 18.16%\n",
      "Epoch: 2, Val Loss: 2.187, Val Acc: 19.46%\n",
      "Epoch: 3, Val Loss: 2.168, Val Acc: 20.61%\n",
      "Epoch: 4, Val Loss: 2.151, Val Acc: 22.06%\n",
      "Epoch: 5, Val Loss: 2.120, Val Acc: 22.96%\n",
      "Epoch: 6, Val Loss: 2.092, Val Acc: 23.61%\n",
      "Epoch: 7, Val Loss: 2.042, Val Acc: 26.61%\n",
      "Epoch: 8, Val Loss: 2.001, Val Acc: 29.06%\n",
      "Epoch: 9, Val Loss: 2.010, Val Acc: 28.41%\n",
      "Epoch: 10, Val Loss: 1.969, Val Acc: 30.47%\n",
      "Test Accuracy: 31.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading history steps 9-10, summary, console lines 11-12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▄▄▆▆▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▂▂▃▄▄▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▇▆▆▅▄▃▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.3105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.30465\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.96876\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_32_fo_doubling_dn_128_ca_gelu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/23drazuo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 20 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_162013-23drazuo/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ozhrmdl7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_163105-ozhrmdl7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33measy-sweep-29\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/ozhrmdl7\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.331, Val Acc: 23.16%\n",
      "Epoch: 2, Val Loss: 1.950, Val Acc: 31.62%\n",
      "Epoch: 3, Val Loss: 1.864, Val Acc: 33.67%\n",
      "Epoch: 4, Val Loss: 1.889, Val Acc: 32.72%\n",
      "Epoch: 5, Val Loss: 1.898, Val Acc: 34.42%\n",
      "Epoch: 6, Val Loss: 1.973, Val Acc: 32.17%\n",
      "Epoch: 7, Val Loss: 1.775, Val Acc: 37.62%\n",
      "Epoch: 8, Val Loss: 1.802, Val Acc: 38.77%\n",
      "Epoch: 9, Val Loss: 1.731, Val Acc: 40.32%\n",
      "Epoch: 10, Val Loss: 1.863, Val Acc: 36.92%\n",
      "Test Accuracy: 38.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▅▆▆▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▄▅▅▆▅▇▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▄▃▃▃▄▂▂▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.382\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.4185\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.36918\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.86326\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_64_fo_doubling_dn_512_ca_relu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/ozhrmdl7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 16 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_163105-ozhrmdl7/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1nhx8dw8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_164236-1nhx8dw8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtoasty-sweep-30\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/1nhx8dw8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.357, Val Acc: 11.61%\n",
      "Epoch: 2, Val Loss: 2.295, Val Acc: 11.31%\n",
      "Epoch: 3, Val Loss: 2.282, Val Acc: 15.01%\n",
      "Epoch: 4, Val Loss: 2.228, Val Acc: 16.71%\n",
      "Epoch: 5, Val Loss: 2.215, Val Acc: 18.31%\n",
      "Epoch: 6, Val Loss: 2.157, Val Acc: 21.61%\n",
      "Epoch: 7, Val Loss: 2.301, Val Acc: 11.26%\n",
      "Epoch: 8, Val Loss: 2.256, Val Acc: 16.11%\n",
      "Epoch: 9, Val Loss: 2.154, Val Acc: 21.51%\n",
      "Epoch: 10, Val Loss: 2.160, Val Acc: 22.41%\n",
      "Test Accuracy: 23.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading history steps 9-10, summary, console lines 11-12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▃▁▁▄▅▆▃▃▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▁▃▄▅▇▁▄▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▅▄▃▁▆▅▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.2315\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.19637\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.22411\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.16003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_64_fo_doubling_dn_256_ca_gelu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/1nhx8dw8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_164236-1nhx8dw8/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uv4jjhk2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_165115-uv4jjhk2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvague-sweep-31\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/uv4jjhk2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 1.963, Val Acc: 30.47%\n",
      "Epoch: 2, Val Loss: 1.966, Val Acc: 30.62%\n",
      "Epoch: 3, Val Loss: 1.894, Val Acc: 34.52%\n",
      "Epoch: 4, Val Loss: 1.837, Val Acc: 34.87%\n",
      "Epoch: 5, Val Loss: 1.851, Val Acc: 37.67%\n",
      "Epoch: 6, Val Loss: 1.980, Val Acc: 34.22%\n",
      "Epoch: 7, Val Loss: 1.760, Val Acc: 39.67%\n",
      "Epoch: 8, Val Loss: 1.811, Val Acc: 40.32%\n",
      "Epoch: 9, Val Loss: 1.940, Val Acc: 37.62%\n",
      "Epoch: 10, Val Loss: 1.856, Val Acc: 39.52%\n",
      "Test Accuracy: 40.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▃▃▄▄▅▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▁▄▄▆▄██▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▇█▅▃▄█▁▃▇▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.4065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.64062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.3952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.85573\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_64_fo_doubling_dn_128_ca_gelu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/uv4jjhk2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 16 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_165115-uv4jjhk2/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: whs443jv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_170010-whs443jv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlaced-sweep-32\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/whs443jv\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.030, Val Acc: 27.41%\n",
      "Epoch: 2, Val Loss: 1.961, Val Acc: 30.37%\n",
      "Epoch: 3, Val Loss: 2.020, Val Acc: 28.06%\n",
      "Epoch: 4, Val Loss: 1.926, Val Acc: 31.37%\n",
      "Epoch: 5, Val Loss: 1.905, Val Acc: 31.87%\n",
      "Epoch: 6, Val Loss: 1.859, Val Acc: 34.17%\n",
      "Epoch: 7, Val Loss: 1.862, Val Acc: 34.52%\n",
      "Epoch: 8, Val Loss: 1.796, Val Acc: 36.82%\n",
      "Epoch: 9, Val Loss: 1.781, Val Acc: 38.47%\n",
      "Epoch: 10, Val Loss: 1.737, Val Acc: 39.32%\n",
      "Test Accuracy: 40.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading history steps 9-10, summary, console lines 11-12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▅▅▅▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▃▁▃▄▅▅▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆█▆▅▄▄▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.4075\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.3955\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.3932\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.73712\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_32_fo_doubling_dn_256_ca_relu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/whs443jv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 20 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_170010-whs443jv/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dyjgw28u with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_171109-dyjgw28u\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mradiant-sweep-33\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/dyjgw28u\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 1.977, Val Acc: 30.87%\n",
      "Epoch: 2, Val Loss: 1.879, Val Acc: 34.12%\n",
      "Epoch: 3, Val Loss: 1.823, Val Acc: 36.77%\n",
      "Epoch: 4, Val Loss: 1.844, Val Acc: 35.77%\n",
      "Epoch: 5, Val Loss: 1.811, Val Acc: 36.92%\n",
      "Epoch: 6, Val Loss: 1.844, Val Acc: 38.47%\n",
      "Epoch: 7, Val Loss: 1.792, Val Acc: 38.57%\n",
      "Epoch: 8, Val Loss: 1.831, Val Acc: 38.32%\n",
      "Epoch: 9, Val Loss: 1.944, Val Acc: 37.52%\n",
      "Epoch: 10, Val Loss: 2.162, Val Acc: 35.82%\n",
      "Test Accuracy: 37.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading history steps 9-10, summary, console lines 11-12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▂▃▃▄▅▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▄▆▅▆███▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▅▃▂▂▁▂▁▂▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.376\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.71788\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.35818\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.16193\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_64_fo_doubling_dn_512_ca_mish_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/dyjgw28u\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_171109-dyjgw28u/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: miimvpm2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_172021-miimvpm2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhappy-sweep-34\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/miimvpm2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 1.997, Val Acc: 29.36%\n",
      "Epoch: 2, Val Loss: 1.928, Val Acc: 31.12%\n",
      "Epoch: 3, Val Loss: 1.815, Val Acc: 35.77%\n",
      "Epoch: 4, Val Loss: 1.828, Val Acc: 35.02%\n",
      "Epoch: 5, Val Loss: 1.757, Val Acc: 36.52%\n",
      "Epoch: 6, Val Loss: 1.765, Val Acc: 38.02%\n",
      "Epoch: 7, Val Loss: 1.769, Val Acc: 37.22%\n",
      "Epoch: 8, Val Loss: 1.845, Val Acc: 38.42%\n",
      "Epoch: 9, Val Loss: 1.778, Val Acc: 40.32%\n",
      "Epoch: 10, Val Loss: 1.879, Val Acc: 38.67%\n",
      "Test Accuracy: 41.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▃▃▄▅▅▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▂▅▅▆▇▆▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▃▃▁▁▁▄▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.4105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.62575\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.38669\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.87928\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_32_fo_doubling_dn_256_ca_gelu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/miimvpm2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 16 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_172021-miimvpm2/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kwok3o77 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_172840-kwok3o77\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mworthy-sweep-35\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/kwok3o77\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.090, Val Acc: 26.46%\n",
      "Epoch: 2, Val Loss: 2.050, Val Acc: 25.56%\n",
      "Epoch: 3, Val Loss: 2.018, Val Acc: 27.06%\n",
      "Epoch: 4, Val Loss: 1.992, Val Acc: 27.86%\n",
      "Epoch: 5, Val Loss: 1.927, Val Acc: 31.92%\n",
      "Epoch: 6, Val Loss: 1.943, Val Acc: 30.02%\n",
      "Epoch: 7, Val Loss: 1.894, Val Acc: 32.52%\n",
      "Epoch: 8, Val Loss: 1.835, Val Acc: 36.37%\n",
      "Epoch: 9, Val Loss: 1.830, Val Acc: 33.67%\n",
      "Epoch: 10, Val Loss: 1.836, Val Acc: 36.37%\n",
      "Test Accuracy: 38.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▅▅▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▂▁▂▂▅▄▆█▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▇▆▅▄▄▃▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.388\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.39125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.36368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.83637\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_64_fo_same_dn_256_ca_silu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/kwok3o77\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_172840-kwok3o77/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: spt0mh2a with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: halving\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_173933-spt0mh2a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcolorful-sweep-36\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/spt0mh2a\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_32_fo_halving_dn_512_ca_relu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/spt0mh2a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_173933-spt0mh2a/logs\u001b[0m\n",
      "Run spt0mh2a errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_19/1168500401.py\", line 326, in sweep_train\n",
      "    train_model(config)\n",
      "  File \"/tmp/ipykernel_19/1168500401.py\", line 234, in train_model\n",
      "    outputs = model(inputs)\n",
      "              ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1844, in _call_impl\n",
      "    return inner()\n",
      "           ^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1790, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_19/3824698835.py\", line 96, in forward\n",
      "    x = conv(x)\n",
      "        ^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.51 GiB is free. Process 3109 has 14.38 GiB memory in use. Of the allocated memory 10.86 GiB is allocated by PyTorch, and 3.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run spt0mh2a errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/1168500401.py\", line 326, in sweep_train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_model(config)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/1168500401.py\", line 234, in train_model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1844, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return inner()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1790, in inner\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     result = forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/3824698835.py\", line 96, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = conv(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.51 GiB is free. Process 3109 has 14.38 GiB memory in use. Of the allocated memory 10.86 GiB is allocated by PyTorch, and 3.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: t5totbk8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_173948-t5totbk8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcool-sweep-37\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/t5totbk8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.157, Val Acc: 20.71%\n",
      "Epoch: 2, Val Loss: 2.044, Val Acc: 27.81%\n",
      "Epoch: 3, Val Loss: 1.999, Val Acc: 28.56%\n",
      "Epoch: 4, Val Loss: 1.911, Val Acc: 32.72%\n",
      "Epoch: 5, Val Loss: 1.870, Val Acc: 34.82%\n",
      "Epoch: 6, Val Loss: 1.909, Val Acc: 31.37%\n",
      "Epoch: 7, Val Loss: 1.840, Val Acc: 34.17%\n",
      "Epoch: 8, Val Loss: 1.847, Val Acc: 34.17%\n",
      "Epoch: 9, Val Loss: 1.792, Val Acc: 37.07%\n",
      "Epoch: 10, Val Loss: 1.796, Val Acc: 36.47%\n",
      "Test Accuracy: 37.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▅▆▆▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▄▄▆▇▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▅▃▃▃▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.374\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.3525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.36468\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.79603\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_32_fo_same_dn_128_ca_gelu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/t5totbk8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_173948-t5totbk8/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4twbwd1v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: halving\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_175032-4twbwd1v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33munique-sweep-38\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/4twbwd1v\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_32_fo_halving_dn_256_ca_relu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/4twbwd1v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_175032-4twbwd1v/logs\u001b[0m\n",
      "Run 4twbwd1v errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_19/1168500401.py\", line 326, in sweep_train\n",
      "    train_model(config)\n",
      "  File \"/tmp/ipykernel_19/1168500401.py\", line 234, in train_model\n",
      "    outputs = model(inputs)\n",
      "              ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1844, in _call_impl\n",
      "    return inner()\n",
      "           ^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1790, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_19/3824698835.py\", line 96, in forward\n",
      "    x = conv(x)\n",
      "        ^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.51 GiB is free. Process 3109 has 14.38 GiB memory in use. Of the allocated memory 10.89 GiB is allocated by PyTorch, and 3.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 4twbwd1v errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/1168500401.py\", line 326, in sweep_train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_model(config)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/1168500401.py\", line 234, in train_model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1844, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return inner()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1790, in inner\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     result = forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/3824698835.py\", line 96, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = conv(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.51 GiB is free. Process 3109 has 14.38 GiB memory in use. Of the allocated memory 10.89 GiB is allocated by PyTorch, and 3.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9vn2pbwa with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: same\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_175047-9vn2pbwa\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mnorthern-sweep-39\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/9vn2pbwa\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.110, Val Acc: 23.76%\n",
      "Epoch: 2, Val Loss: 2.058, Val Acc: 24.71%\n",
      "Epoch: 3, Val Loss: 2.008, Val Acc: 28.41%\n",
      "Epoch: 4, Val Loss: 1.959, Val Acc: 30.32%\n",
      "Epoch: 5, Val Loss: 1.940, Val Acc: 30.72%\n",
      "Epoch: 6, Val Loss: 1.985, Val Acc: 31.12%\n",
      "Epoch: 7, Val Loss: 1.955, Val Acc: 31.62%\n",
      "Epoch: 8, Val Loss: 1.991, Val Acc: 32.17%\n",
      "Epoch: 9, Val Loss: 2.081, Val Acc: 30.77%\n",
      "Epoch: 10, Val Loss: 2.323, Val Acc: 30.52%\n",
      "Test Accuracy: 34.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▃▃▄▄▅▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▂▅▆▇▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▄▃▂▁▁▂▁▂▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.3405\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.57137\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.30515\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.32348\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_32_fo_same_dn_256_ca_relu_da_leakyrelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/9vn2pbwa\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 18 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_175047-9vn2pbwa/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7kayosnx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: doubling\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250418_175855-7kayosnx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvisionary-sweep-40\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/sweeps/owlu7slq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/7kayosnx\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8000\n",
      "Total validation samples: 1999\n",
      "Epoch: 1, Val Loss: 2.277, Val Acc: 14.61%\n",
      "Epoch: 2, Val Loss: 2.303, Val Acc: 10.01%\n",
      "Epoch: 3, Val Loss: 2.303, Val Acc: 10.01%\n",
      "Epoch: 4, Val Loss: 2.303, Val Acc: 10.01%\n",
      "Epoch: 5, Val Loss: 2.303, Val Acc: 10.01%\n",
      "Epoch: 6, Val Loss: 2.303, Val Acc: 10.01%\n",
      "Epoch: 7, Val Loss: 2.303, Val Acc: 10.01%\n",
      "Epoch: 8, Val Loss: 2.303, Val Acc: 10.01%\n",
      "Epoch: 9, Val Loss: 2.303, Val Acc: 9.95%\n",
      "Epoch: 10, Val Loss: 2.303, Val Acc: 10.01%\n",
      "Test Accuracy: 10.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model; uploading history steps 9-10, summary, console lines 11-12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy █▇▂▃▂▂▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy █▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▁█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.09362\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.10005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.30254\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbf_64_fo_doubling_dn_256_ca_gelu_da_gelu_v5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/7kayosnx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m008-iit-madras/DA6401-A2-V4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250418_175855-7kayosnx/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Initialize sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"DA6401-A2-V4\")\n",
    "\n",
    "# Start the sweep\n",
    "wandb.agent(sweep_id, sweep_train, count=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bf7b52",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b4e28f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Path to the test data\n",
    "DATASET_PATH = r\"E:\\IITM\\2nd sem\\inaturalist_12K\"  # Update with your path\n",
    "TEST_DIR = os.path.join(DATASET_PATH, \"val\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Image size\n",
    "IMAGE_SIZE = 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94859c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_run_from_sweep(sweep_id, entity, project):\n",
    "    api = wandb.Api()\n",
    "    \n",
    "    sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "    \n",
    "    best_val_acc = -1\n",
    "    best_run = None\n",
    "    \n",
    "    for run in sweep.runs:\n",
    "        val_acc = run.summary.get(\"val_accuracy\")\n",
    "        if val_acc is not None and val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_run = run\n",
    "    \n",
    "    if best_run is None:\n",
    "        raise ValueError(\"No runs with 'val_accuracy' found in the sweep\")\n",
    "    \n",
    "    print(f\"Best run: {best_run.name}, val_accuracy: {best_val_acc:.4f}\")\n",
    "    return best_run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce51739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_best_model(run, artifact_name=\"model\", output_dir=\"downloaded_model\"):\n",
    "    \"\"\"\n",
    "    Download the best model file from wandb.\n",
    "    \n",
    "    Args:\n",
    "        best_run: The wandb run object for the best run\n",
    "    \n",
    "    Returns:\n",
    "        model_path: Local path to the downloaded model\n",
    "        config: Configuration of the best model\n",
    "    \"\"\"\n",
    "    api = wandb.Api()\n",
    "\n",
    "    # List all artifacts with this name in the project\n",
    "    artifact_versions = api.artifacts(name=f\"{run.project}/{artifact_name}\", type_name='model')\n",
    "    output_dir = os.path.join(os.getcwd(), output_dir)\n",
    "    for artifact in artifact_versions:\n",
    "        # Match the artifact to the run that created it\n",
    "        if artifact.logged_by and artifact.logged_by().id == run.id:\n",
    "            print(f\"Found artifact version: {artifact.version} from run: {run.name}\")\n",
    "            artifact_dir = artifact.download(root=output_dir)\n",
    "            for file_name in os.listdir(artifact_dir):\n",
    "                print(file_name)\n",
    "                if file_name.startswith(\"final_model\") and file_name.endswith(\".pth\"):\n",
    "                    model_path = os.path.join(os.getcwd(), artifact_dir, file_name)\n",
    "                    print(f\"Downloaded model file: {model_path}\")\n",
    "    \n",
    "    print(f\"Downloaded model file: {model_path}\")\n",
    "    \n",
    "    # Get the model configuration from the run\n",
    "    config = {\n",
    "        'base_filters': run.config.get('base_filters', 32),\n",
    "        'dense_activation': run.config.get('dense_activation', 'relu'),\n",
    "        'filter_organization': run.config.get('filter_organization', 'doubling'),\n",
    "        'dense_neurons': run.config.get('dense_neurons', 512),\n",
    "        'dropout_rate': run.config.get('dropout_rate', 0.3),\n",
    "        'use_batch_norm': run.config.get('use_batch_norm', True),\n",
    "        'conv_activation': run.config.get('conv_activation', 'mish')\n",
    "    }\n",
    "    \n",
    "    return model_path, config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a364948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    \"\"\"Load the test dataset\"\"\"\n",
    "    # Define transforms for test data\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load test dataset\n",
    "    test_dataset = iNaturalistDataset(root_dir=TEST_DIR, transform=test_transform)\n",
    "    \n",
    "    # Create data loader\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return test_loader, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c7b6826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(sweep_id='uf2dfd5t', entity='da24m008-iit-madras', project='DA6401-A2-V4'):\n",
    "    \"\"\"Evaluate the best model on the test set\"\"\"\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=project, job_type=\"evaluation\")\n",
    "    \n",
    "    try:\n",
    "        # Get the best run and download its model\n",
    "        best_run = get_best_run_from_sweep(\n",
    "            sweep_id=sweep_id,\n",
    "            entity=entity,\n",
    "            project=project\n",
    "        )\n",
    "\n",
    "        model_path, best_config = download_best_model(best_run)\n",
    "        \n",
    "        # Log the best run information\n",
    "        wandb.log({\"best_run_id\": best_run.id, \"best_run_name\": best_run.name})\n",
    "        \n",
    "        # Load test data\n",
    "        test_loader, test_dataset = load_test_data()\n",
    "        \n",
    "        # Get class names and count\n",
    "        class_names = test_dataset.classes\n",
    "        num_classes = len(class_names)\n",
    "        print(f\"Number of classes: {num_classes}\")\n",
    "        print(f\"Class names: {class_names}\")\n",
    "        \n",
    "        # Update config with the correct number of classes\n",
    "        best_config['num_classes'] = num_classes\n",
    "        \n",
    "        # Create model with the best configuration\n",
    "        if best_config['filter_organization'] == 'same':\n",
    "            filters = [best_config['base_filters']] * 5\n",
    "        elif best_config['filter_organization'] == 'doubling':\n",
    "            filters = [best_config['base_filters'] * (2**i) for i in range(5)]\n",
    "        elif best_config['filter_organization'] == 'halving':\n",
    "            filters = [best_config['base_filters'] * (2**(4-i)) for i in range(5)]\n",
    "        else:\n",
    "            filters = [32, 64, 128, 256, 512]  # Default\n",
    "        \n",
    "        model = CNNModel(\n",
    "            input_channels=3,\n",
    "            num_classes=num_classes,\n",
    "            filters_per_layer=filters,\n",
    "            kernel_size=3,\n",
    "            conv_activation=best_config['conv_activation'],\n",
    "            dense_units=best_config['dense_neurons'],\n",
    "            dropout_rate=best_config['dropout_rate'],\n",
    "            use_batch_norm=best_config['use_batch_norm'],\n",
    "            dense_activation=best_config['dense_activation']\n",
    "        )\n",
    "        \n",
    "        # Load the best model weights\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model = model.to(device)\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        # Evaluate model\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                # Store for confusion matrix\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        test_accuracy = correct / total\n",
    "        print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log({\n",
    "            \"best_model_test_accuracy\": test_accuracy\n",
    "        })\n",
    "        \n",
    "        return model, test_loader, test_dataset, all_labels, all_predictions, class_names\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96cb90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_grid(model, test_dataset, class_names):\n",
    "    \"\"\"Create a 10x3 grid of test images with predictions\"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Define transform for visualization\n",
    "    vis_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Sample indices for the grid\n",
    "    num_samples = min(30, len(test_dataset))  # 10x3 grid needs 30 images\n",
    "    indices = np.random.choice(len(test_dataset), num_samples, replace=False)\n",
    "    \n",
    "    # Create figure for the grid\n",
    "    plt.figure(figsize=(15, 25))\n",
    "    \n",
    "    # Create lists to store images and captions for wandb\n",
    "    wandb_images = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            # Get image and label\n",
    "            original_image, label = test_dataset[idx]\n",
    "            \n",
    "            # For visualization, we need the unnormalized image\n",
    "            img_path = test_dataset.image_paths[idx]\n",
    "            vis_image = Image.open(img_path).convert('RGB')\n",
    "            vis_tensor = vis_transform(vis_image)\n",
    "            \n",
    "            # Move to device and add batch dimension\n",
    "            input_tensor = original_image.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get prediction\n",
    "            output = model(input_tensor)\n",
    "            _, prediction = output.max(1)\n",
    "            prediction = prediction.item()\n",
    "            \n",
    "            # Plot\n",
    "            plt.subplot(10, 3, i+1)\n",
    "            # Convert tensor to numpy for plotting\n",
    "            img_array = vis_tensor.permute(1, 2, 0).numpy()\n",
    "            plt.imshow(img_array)\n",
    "            \n",
    "            true_class = class_names[label]\n",
    "            pred_class = class_names[prediction]\n",
    "            \n",
    "            if label == prediction:\n",
    "                color = 'green'\n",
    "                caption = f\"True: {true_class} | Pred: {pred_class} ✓\"\n",
    "            else:\n",
    "                color = 'red'\n",
    "                caption = f\"True: {true_class} | Pred: {pred_class} ✗\"\n",
    "            \n",
    "            plt.title(caption, color=color)\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Add to wandb images list\n",
    "            wandb_images.append(wandb.Image(img_array, caption=caption))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig('prediction_grid.png')\n",
    "    \n",
    "    # Log the figure to wandb\n",
    "    wandb.log({\"prediction_grid\": wandb.Image('prediction_grid.png')})\n",
    "    \n",
    "    # Also log the individual images with captions\n",
    "    wandb.log({\"test_predictions\": wandb_images})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d39a78b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(all_labels, all_predictions, class_names):\n",
    "    \"\"\"Create and log a confusion matrix\"\"\"\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    \n",
    "    # Log the figure to wandb\n",
    "    wandb.log({\"confusion_matrix\": wandb.Image('confusion_matrix.png')})\n",
    "    \n",
    "    # Also log a summary of class-wise accuracies\n",
    "    class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "    for i, (class_name, accuracy) in enumerate(zip(class_names, class_accuracy)):\n",
    "        wandb.log({f\"class_accuracy/{class_name}\": accuracy})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c9bdfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_report(all_labels, all_predictions, class_names):\n",
    "    \"\"\"Generate and log classification report\"\"\"    \n",
    "    # Generate report\n",
    "    report = classification_report(all_labels, all_predictions, \n",
    "                                  target_names=class_names, \n",
    "                                  output_dict=True)\n",
    "    \n",
    "    # Log to wandb\n",
    "    for class_name in class_names:\n",
    "        if class_name in report:\n",
    "            wandb.log({\n",
    "                f\"metrics/{class_name}/precision\": report[class_name]['precision'],\n",
    "                f\"metrics/{class_name}/recall\": report[class_name]['recall'],\n",
    "                f\"metrics/{class_name}/f1-score\": report[class_name]['f1-score']\n",
    "            })\n",
    "    \n",
    "    # Log overall metrics\n",
    "    wandb.log({\n",
    "        \"metrics/accuracy\": report['accuracy'],\n",
    "        \"metrics/macro_avg_precision\": report['macro avg']['precision'],\n",
    "        \"metrics/macro_avg_recall\": report['macro avg']['recall'],\n",
    "        \"metrics/macro_avg_f1\": report['macro avg']['f1-score'],\n",
    "        \"metrics/weighted_avg_precision\": report['weighted avg']['precision'],\n",
    "        \"metrics/weighted_avg_recall\": report['weighted avg']['recall'],\n",
    "        \"metrics/weighted_avg_f1\": report['weighted avg']['f1-score']\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd18c7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\IITM\\2nd sem\\DL\\partA\\wandb\\run-20250419_201920-5jv896vf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/5jv896vf' target=\"_blank\">warm-jazz-145</a></strong> to <a href='https://wandb.ai/da24m008-iit-madras/DA6401-A2-V4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m008-iit-madras/DA6401-A2-V4' target=\"_blank\">https://wandb.ai/da24m008-iit-madras/DA6401-A2-V4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/5jv896vf' target=\"_blank\">https://wandb.ai/da24m008-iit-madras/DA6401-A2-V4/runs/5jv896vf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run: bf_32_fo_doubling_dn_512_ca_mish_da_gelu_v5, val_accuracy: 0.4287\n",
      "Found artifact version: v69 from run: bf_32_fo_doubling_dn_512_ca_mish_da_gelu_v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Downloading large artifact model:v69, 55.04MB. 1 files... \n",
      "wandb:   1 of 1 files downloaded.  \n",
      "Done. 0:0:3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_73jfitab.pth\n",
      "final_model_73jfitab.pth\n",
      "Downloaded model file: e:\\IITM\\2nd sem\\DL\\partA\\downloaded_model\\final_model_73jfitab.pth\n",
      "Found artifact version: v68 from run: bf_32_fo_doubling_dn_512_ca_mish_da_gelu_v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Downloading large artifact model:v68, 55.04MB. 1 files... \n",
      "wandb:   1 of 1 files downloaded.  \n",
      "Done. 0:0:3.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_73jfitab.pth\n",
      "final_model_73jfitab.pth\n",
      "Downloaded model file: e:\\IITM\\2nd sem\\DL\\partA\\downloaded_model\\final_model_73jfitab.pth\n",
      "Found artifact version: v67 from run: bf_32_fo_doubling_dn_512_ca_mish_da_gelu_v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Downloading large artifact model:v67, 55.04MB. 1 files... \n",
      "wandb:   1 of 1 files downloaded.  \n",
      "Done. 0:0:1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_73jfitab.pth\n",
      "final_model_73jfitab.pth\n",
      "Downloaded model file: e:\\IITM\\2nd sem\\DL\\partA\\downloaded_model\\final_model_73jfitab.pth\n",
      "Found artifact version: v66 from run: bf_32_fo_doubling_dn_512_ca_mish_da_gelu_v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Downloading large artifact model:v66, 55.04MB. 1 files... \n",
      "wandb:   1 of 1 files downloaded.  \n",
      "Done. 0:0:1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_73jfitab.pth\n",
      "final_model_73jfitab.pth\n",
      "Downloaded model file: e:\\IITM\\2nd sem\\DL\\partA\\downloaded_model\\final_model_73jfitab.pth\n",
      "Found artifact version: v65 from run: bf_32_fo_doubling_dn_512_ca_mish_da_gelu_v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Downloading large artifact model:v65, 55.04MB. 1 files... \n",
      "wandb:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_73jfitab.pth\n",
      "final_model_73jfitab.pth\n",
      "Downloaded model file: e:\\IITM\\2nd sem\\DL\\partA\\downloaded_model\\final_model_73jfitab.pth\n",
      "Found artifact version: v64 from run: bf_32_fo_doubling_dn_512_ca_mish_da_gelu_v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Downloading large artifact model:v64, 55.04MB. 1 files... \n",
      "wandb:   1 of 1 files downloaded.  \n",
      "Done. 0:0:1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_73jfitab.pth\n",
      "final_model_73jfitab.pth\n",
      "Downloaded model file: e:\\IITM\\2nd sem\\DL\\partA\\downloaded_model\\final_model_73jfitab.pth\n",
      "Found artifact version: v63 from run: bf_32_fo_doubling_dn_512_ca_mish_da_gelu_v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Downloading large artifact model:v63, 55.04MB. 1 files... \n",
      "wandb:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_73jfitab.pth\n",
      "final_model_73jfitab.pth\n",
      "Downloaded model file: e:\\IITM\\2nd sem\\DL\\partA\\downloaded_model\\final_model_73jfitab.pth\n",
      "Downloaded model file: e:\\IITM\\2nd sem\\DL\\partA\\downloaded_model\\final_model_73jfitab.pth\n",
      "Number of classes: 10\n",
      "Class names: ['Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi', 'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "model, test_loader, test_dataset, all_labels, all_predictions, class_names = evaluate_model(\n",
    "    sweep_id='uf2dfd5t',\n",
    "    entity='da24m008-iit-madras',\n",
    "    project='DA6401-A2-V4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3021846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction grid\n",
    "create_prediction_grid(model, test_dataset, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6289d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "create_confusion_matrix(all_labels, all_predictions, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "generate_classification_report(all_labels, all_predictions, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a687b40a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7171620,
     "sourceId": 11447125,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20907.560779,
   "end_time": "2025-04-18T18:07:34.642727",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-18T12:19:07.081948",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
